{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24a9cbc9",
   "metadata": {},
   "source": [
    "Harika, o zaman **yaptÄ±ÄŸÄ±n tÃ¼m sÃ¼reci ve son eklemeyi tek bir akÄ±ÅŸ hÃ¢linde** Ã¶zetleyelim. Bu, hem ne yaptÄ±ÄŸÄ±nÄ± hem de bundan sonra nasÄ±l kullanacaÄŸÄ±nÄ± net gÃ¶sterir.\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£ OrtamÄ± hazÄ±rladÄ±n (M2 Mac + Python venv)\n",
    "\n",
    "**Ne yaptÄ±n:**\n",
    "\n",
    "1. Mevcut environmentâ€™Ä± kapattÄ±n:\n",
    "\n",
    "```bash\n",
    "deactivate\n",
    "```\n",
    "\n",
    "2. Temiz bir virtual environment oluÅŸturdun:\n",
    "\n",
    "```bash\n",
    "python3 -m venv my_steps_env\n",
    "source my_steps_env/bin/activate\n",
    "```\n",
    "\n",
    "3. Paketleri yÃ¼kledin ve uyumlu hale getirdin:\n",
    "\n",
    "```bash\n",
    "pip install --upgrade pip setuptools wheel\n",
    "pip install torch torchvision torchaudio      # PyTorch â‰¥2.6, M2 Metal uyumlu\n",
    "pip install \"numpy<2\"                         # NumPy uyumlu\n",
    "pip install sentence-transformers transformers python-dotenv pinecone grpcio protobuf googleapis-common-protos\n",
    "pip install openai                             # opsiyonel, artÄ±k lokal LLM ile de Ã§alÄ±ÅŸÄ±yor\n",
    "```\n",
    "\n",
    "**Neden:**\n",
    "\n",
    "* PyTorch â‰¥2.6 â†’ M2 Mac Metal GPU uyumlu\n",
    "* NumPy <2 â†’ sentence-transformers uyumu\n",
    "* Pinecone v3 + gRPC Ã§alÄ±ÅŸmasÄ± iÃ§in protobuf ve grpcio gerekli\n",
    "\n",
    "---\n",
    "\n",
    "# 2ï¸âƒ£ .env dosyasÄ± ile Pinecone API key ekledin\n",
    "\n",
    "```\n",
    "PINECONE_API_KEY=senin_api_key\n",
    "```\n",
    "\n",
    "* Pineconeâ€™a baÄŸlanabilmek iÃ§in API key ÅŸart\n",
    "* `load_dotenv()` ile scriptâ€™ine Ã§ekiyorsun\n",
    "\n",
    "---\n",
    "\n",
    "# 3ï¸âƒ£ Pinecone index oluÅŸturup kontrol ettin\n",
    "\n",
    "```python\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "if \"rag-demo\" not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=\"rag-demo\",\n",
    "        dimension=384,         # BGE-small embedding boyutu\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "```\n",
    "\n",
    "* Index = veri tabanÄ±, embeddingâ€™leri burada saklÄ±yorsun\n",
    "* Boyut (dimension) embedding modeli ile aynÄ± olmalÄ±\n",
    "\n",
    "---\n",
    "\n",
    "# 4ï¸âƒ£ Embedding modelini yÃ¼kledin\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(\"BAAI/bge-small-en\", device=device)\n",
    "```\n",
    "\n",
    "* BGE-small â†’ metinleri vektÃ¶re (embedding) Ã§eviriyor\n",
    "* M2 Metal GPU varsa kullanÄ±yor, yoksa CPU fallback\n",
    "\n",
    "---\n",
    "\n",
    "# 5ï¸âƒ£ DokÃ¼manlarÄ± embed edip Pineconeâ€™a upsert ettin\n",
    "\n",
    "```python\n",
    "docs = [(\"doc1\", \"FastAPI is a Python web framework.\"),\n",
    "        (\"doc2\", \"Docker containers are used for packaging apps.\")]\n",
    "\n",
    "vectors = [(doc_id, model.encode(text).tolist(), {\"text\": text}) for doc_id, text in docs]\n",
    "pc.upsert(index_name=\"rag-demo\", vectors=vectors)\n",
    "```\n",
    "\n",
    "* Metadata ile metin iÃ§eriÄŸini sakladÄ±n\n",
    "* ArtÄ±k query ile eÅŸleÅŸme bulabilir hale geldi\n",
    "\n",
    "---\n",
    "\n",
    "# 6ï¸âƒ£ Query yaptÄ±n ve benzer dokÃ¼manlarÄ± aldÄ±n\n",
    "\n",
    "```python\n",
    "query = \"What is FastAPI?\"\n",
    "q_vec = model.encode(query).tolist()\n",
    "res = pc.query(index_name=\"rag-demo\", vector=q_vec, top_k=2, include_metadata=True)\n",
    "print(res)\n",
    "```\n",
    "\n",
    "* `score` â†’ dokÃ¼manÄ±n query ile benzerliÄŸi\n",
    "* `metadata` â†’ orijinal metin\n",
    "\n",
    "---\n",
    "\n",
    "# 7ï¸âƒ£ RAG pipeline: Query â†’ Context â†’ Prompt â†’ LLM response (yeni ekleme)\n",
    "\n",
    "Sen OpenAI kullanmak yerine **Flan-T5 (seq2seq)** kullandÄ±n:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "model_name = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "llm_pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n",
    "\n",
    "# Context oluÅŸtur\n",
    "context = \"\\n\".join([match['metadata']['text'] for match in res['matches']])\n",
    "prompt = \"Answer based on context:\\n\" + context + \"\\nQuestion: \" + query\n",
    "\n",
    "# LLM response\n",
    "answer = llm_pipe(prompt)[0]['generated_text']\n",
    "\n",
    "print(\"\\n--- Context from Pinecone ---\\n\", context)\n",
    "print(\"\\n--- LLM Answer ---\\n\", answer)\n",
    "```\n",
    "\n",
    "**Ne oldu:**\n",
    "\n",
    "* Pineconeâ€™dan top_k dokÃ¼manlarÄ± aldÄ±n\n",
    "* Context oluÅŸturup prompt hazÄ±rlandÄ±\n",
    "* Flan-T5 modelinden yanÄ±t Ã¼retildi\n",
    "* ArtÄ±k **tamamen Ã¼cretsiz ve local LLM** ile RAG pipeline Ã§alÄ±ÅŸÄ±yor\n",
    "\n",
    "---\n",
    "\n",
    "# 8ï¸âƒ£ Bundan sonra nasÄ±l kullanacaksÄ±n\n",
    "\n",
    "1. **FastAPI ile API oluÅŸtur:**\n",
    "\n",
    "   * KullanÄ±cÄ± query gÃ¶nderir â†’ embedding â†’ Pinecone â†’ top_k â†’ context â†’ Flan-T5 â†’ yanÄ±t dÃ¶ner\n",
    "\n",
    "2. **Dockerize et:**\n",
    "\n",
    "   * Tek scriptâ€™i container iÃ§ine koy â†’ RAG service olarak deploy\n",
    "\n",
    "3. **DokÃ¼manlarÄ± artÄ±r ve test et:**\n",
    "\n",
    "   * FarklÄ± namespaceâ€™ler ile Pinecone limitlerini aÅŸmadan bÃ¼yÃ¼t\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ”¹ Ã–zet\n",
    "\n",
    "* Ortam â†’ hazÄ±r ve uyumlu\n",
    "* Pinecone â†’ vektÃ¶r depolama ve retrieval\n",
    "* Embedding â†’ BGE-small\n",
    "* LLM â†’ Flan-T5 (local, Ã¼cretsiz)\n",
    "* Pipeline â†’ query â†’ embedding â†’ Pinecone â†’ context â†’ prompt â†’ LLM response\n",
    "\n",
    "Bu noktada pipeline **Ã§alÄ±ÅŸÄ±yor ve test edilebilir** durumda.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4230c3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
