# RAG Pipeline ile Local LLM KullanÄ±mÄ±

Bu rehber, M2 Mac Ã¼zerinde **Pinecone + BGE-small + Flan-T5** kullanarak **RAG (Retrieval-Augmented Generation) pipeline** kurulumunu ve kullanÄ±mÄ±nÄ± adÄ±m adÄ±m gÃ¶sterir. ArtÄ±k tamamen Ã¼cretsiz ve local LLM ile Ã§alÄ±ÅŸÄ±yor.

---

## ğŸš€ 1. OrtamÄ± HazÄ±rlama (M2 Mac + Python venv)

Mevcut environmentâ€™Ä± kapatÄ±p, temiz bir virtual environment oluÅŸtur:

```bash
deactivate
python3 -m venv my_steps_env
source my_steps_env/bin/activate
````

Gerekli paketleri yÃ¼kle:

```bash
pip install --upgrade pip setuptools wheel
pip install torch torchvision torchaudio      # PyTorch â‰¥2.6, M2 Metal uyumlu
pip install "numpy<2"                         # NumPy uyumlu
pip install sentence-transformers transformers python-dotenv pinecone grpcio protobuf googleapis-common-protos
pip install openai                             # opsiyonel
```

**Notlar:**

* PyTorch â‰¥2.6 â†’ M2 Metal GPU uyumlu
* NumPy <2 â†’ sentence-transformers uyumu
* Pinecone v3 + gRPC Ã§alÄ±ÅŸmasÄ± iÃ§in protobuf ve grpcio gerekli

---

## ğŸ”‘ 2. Pinecone API Key

`.env` dosyasÄ±na ekle:

```env
PINECONE_API_KEY=senin_api_key
```

Scriptâ€™te Ã§ek:

```python
from dotenv import load_dotenv
import os

load_dotenv()
api_key = os.getenv("PINECONE_API_KEY")
```

---

## ğŸ“¦ 3. Pinecone Index OluÅŸturma

```python
from pinecone import Pinecone, ServerlessSpec

pc = Pinecone(api_key=api_key)

if "rag-demo" not in pc.list_indexes().names():
    pc.create_index(
        name="rag-demo",
        dimension=384,         # BGE-small embedding boyutu
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )
```

> Index, embeddingâ€™leri saklayacaÄŸÄ±n veri tabanÄ±dÄ±r. Dimension, embedding modeli ile aynÄ± olmalÄ±.

---

## ğŸ“ 4. Embedding Modelini YÃ¼kleme

```python
from sentence_transformers import SentenceTransformer
import torch

device = "mps" if torch.backends.mps.is_available() else "cpu"
model = SentenceTransformer("BAAI/bge-small-en", device=device)
```

* BGE-small â†’ metinleri vektÃ¶re Ã§evirir
* M2 Metal GPU varsa kullanÄ±r, yoksa CPU fallback

---

## ğŸ“„ 5. DokÃ¼manlarÄ± Embed Edip Pineconeâ€™a Upsert Etme

```python
docs = [("doc1", "FastAPI is a Python web framework."),
        ("doc2", "Docker containers are used for packaging apps.")]

vectors = [(doc_id, model.encode(text).tolist(), {"text": text}) for doc_id, text in docs]
pc.upsert(index_name="rag-demo", vectors=vectors)
```

* Metadata ile metin iÃ§eriÄŸi saklanÄ±r
* ArtÄ±k query ile eÅŸleÅŸme bulabilir hÃ¢le geldi

---

## ğŸ” 6. Query ve Benzer DokÃ¼manlar

```python
query = "What is FastAPI?"
q_vec = model.encode(query).tolist()
res = pc.query(index_name="rag-demo", vector=q_vec, top_k=2, include_metadata=True)
print(res)
```

* `score` â†’ dokÃ¼manÄ±n query ile benzerliÄŸi
* `metadata` â†’ orijinal metin

---

## ğŸ¤– 7. RAG Pipeline: Context + LLM YanÄ±tÄ±

Local LLM olarak **Flan-T5** kullan:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

model_name = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
llm_pipe = pipeline("text2text-generation", model=model, tokenizer=tokenizer, max_length=512)

# Context oluÅŸtur
context = "\n".join([match['metadata']['text'] for match in res['matches']])
prompt = "Answer based on context:\n" + context + "\nQuestion: " + query

# LLM response
answer = llm_pipe(prompt)[0]['generated_text']

print("\n--- Context from Pinecone ---\n", context)
print("\n--- LLM Answer ---\n", answer)
```

**SonuÃ§:**

* Pineconeâ€™dan top_k dokÃ¼manlarÄ± aldÄ±n
* Context oluÅŸturup prompt hazÄ±rlandÄ±
* Flan-T5 modelinden yanÄ±t Ã¼retildi
* Tamamen Ã¼cretsiz ve local LLM ile RAG pipeline Ã§alÄ±ÅŸÄ±yor

---

## âš¡ 8. Bundan Sonra KullanÄ±m

1. **FastAPI ile API oluÅŸtur:**
   KullanÄ±cÄ± query gÃ¶nderir â†’ embedding â†’ Pinecone â†’ top_k â†’ context â†’ Flan-T5 â†’ yanÄ±t dÃ¶ner

2. **Dockerize et:**
   Tek scriptâ€™i container iÃ§ine koy â†’ RAG service olarak deploy

3. **DokÃ¼manlarÄ± artÄ±r ve test et:**
   FarklÄ± namespaceâ€™ler ile Pinecone limitlerini aÅŸmadan bÃ¼yÃ¼t

---

## ğŸ”¹ Ã–zet

| AdÄ±m      | KullanÄ±lan Teknoloji                                           |
| --------- | -------------------------------------------------------------- |
| Ortam     | Python venv, M2 Metal GPU                                      |
| Pinecone  | VektÃ¶r depolama & retrieval                                    |
| Embedding | BGE-small                                                      |
| LLM       | Flan-T5 (local, Ã¼cretsiz)                                      |
| Pipeline  | query â†’ embedding â†’ Pinecone â†’ context â†’ prompt â†’ LLM response |

Pipeline **Ã§alÄ±ÅŸÄ±yor ve test edilebilir** durumda.

---

### ğŸ“¸ Ã–nerilen GÃ¶rseller (Opsiyonel)

1. `pipeline.png` â†’ RAG pipeline akÄ±ÅŸ diyagramÄ±
2. `pinecone_query.png` â†’ Pinecone sorgu Ã§Ä±ktÄ±sÄ±
3. `flan_t5_answer.png` â†’ LLM yanÄ±t ekran gÃ¶rÃ¼ntÃ¼sÃ¼

